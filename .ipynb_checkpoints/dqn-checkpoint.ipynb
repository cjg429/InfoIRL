{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import gym_coverage\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 50000\n",
    "DISPLAY_REWARD_THRESHOLD = -10  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 500 # maximum time step in one episode\n",
    "RENDER = False # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR = 0.0001    # learning rate\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "env = gym.make('Coverage-v0')\n",
    "env.seed(1)  # reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)  # reproducible\n",
    "\n",
    "N_F_pos = env.observation_space.spaces[0].n\n",
    "N_F_map = env.observation_space.spaces[1].shape[0] * env.observation_space.spaces[1].shape[1]\n",
    "N_F = N_F_pos + N_F_map\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay(object):\n",
    "    def __init__(self, memory_size, state_dim, act_dim, batch_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.actions = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.rewards = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.next_states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "    def fifo(self, state, action, reward, next_state):\n",
    "        self.states[self.current] = state\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.next_states[self.current] = next_state\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        self.count = self.count + 1\n",
    "    def add_trajectory(self, states, actions, rewards, next_states):\n",
    "        num = len(observes)\n",
    "        for i in range(0, num):\n",
    "            self.fifo(states[i], actions[i], rewards[i], next_states[i])\n",
    "    def sampling(self):\n",
    "        indexes = np.random.randint(min(self.count, self.memory_size), size=self.batch_size)\n",
    "        states = self.states[indexes]\n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        next_states = self.next_states[indexes]\n",
    "        states = states.reshape((-1, self.state_dim))\n",
    "        actions = actions.reshape((-1, 1))\n",
    "        rewards = rewards.reshape((-1, 1))\n",
    "        next_states = next_states.reshape((-1, self.state_dim))\n",
    "        return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNET(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, (None, n_features), \"state\")\n",
    "        self.r = tf.placeholder(tf.float32, (None, 1), 'reward')\n",
    "        self.q_ = tf.placeholder(tf.float32, (None, 1), 'q_next')\n",
    "        self.td_error = tf.placeholder(tf.float32, (None, 1), 'td_error')\n",
    "\n",
    "        with tf.variable_scope('Q'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=32,  # number of hidden units\n",
    "                activation=tf.nn.tanh,  # None\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "            \n",
    "            l2 = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=16,  # number of hidden units\n",
    "                activation=tf.nn.tanh,  # None\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l2'\n",
    "            )\n",
    "\n",
    "            self.q = tf.layers.dense(\n",
    "                inputs=l2,\n",
    "                units=n_actions,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='Q'\n",
    "            )\n",
    "        \n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            #self.td_error = self.r + GAMMA * self.q_ - self.q[a]\n",
    "            self.loss = tf.reduce_sum(tf.square(self.td_error))    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "    '''\n",
    "    def learn(self, s, a, r, s_):\n",
    "        BATCH_SIZE = 200\n",
    "        NITER = int(s.shape[0]/BATCH_SIZE)\n",
    "        td_error = self.get_error(s, a, r, s_)\n",
    "        for i in range(0, NITER):\n",
    "            _ = self.sess.run(self.train_op, {self.td_error: td_error[i*BATCH_SIZE:(i+1)*BATCH_SIZE]})\n",
    "        return 0\n",
    "    '''\n",
    "    def get_error(self, s, a, r, s_):\n",
    "        q_ = self.sess.run(self.q, {self.s: s_})\n",
    "        q = self.sess.run(self.q, {self.s: s})\n",
    "        td_error = r + gamma * np.amax(q_, axis=1) - q[a]\n",
    "        BATCH_SIZE = 200\n",
    "        NITER = int(s.shape[0]/BATCH_SIZE)\n",
    "        for i in range(0, NITER):\n",
    "            _ = self.sess.run(self.train_op, {self.td_error: td_error[i*BATCH_SIZE:(i+1)*BATCH_SIZE]})\n",
    "        return 0\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        q = self.sess.run(self.q, {self.s: s})\n",
    "        action = np.argmax(q, axis=1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Q/l1/kernel:0' shape=(200, 32) dtype=float32_ref>\", \"<tf.Variable 'Q/l1/bias:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/kernel:0' shape=(32, 16) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/bias:0' shape=(16,) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/kernel:0' shape=(16, 4) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/bias:0' shape=(4,) dtype=float32_ref>\"] and loss Tensor(\"squared_TD_error/Sum:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1378539017ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mqnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# we need a good teacher, so the teacher should learn faster than the actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpReplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEMORY_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-abe123cd3769>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, n_features, n_actions, lr)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# TD_error = (r+gamma*V_next) - V_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cjg429/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    348\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Q/l1/kernel:0' shape=(200, 32) dtype=float32_ref>\", \"<tf.Variable 'Q/l1/bias:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/kernel:0' shape=(32, 16) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/bias:0' shape=(16,) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/kernel:0' shape=(16, 4) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/bias:0' shape=(4,) dtype=float32_ref>\"] and loss Tensor(\"squared_TD_error/Sum:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "qnet = QNET(sess, n_features=N_F, n_actions=N_A, lr=LR)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "replay = ExpReplay(MEMORY_SIZE, N_F, N_A)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    s = np.concatenate((np.eye(100)[s[0]], s[1].flatten()))\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = qnet.get_actions(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.concatenate((np.eye(100)[s_[0]], s_[1].flatten()))\n",
    "        track_r.append(r)\n",
    "       \n",
    "        replay.fifo(s, a, r, s_)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            running_reward = ep_rs_sum\n",
    "            '''\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            '''\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break\n",
    "        \n",
    "    if (i_episode + 1)%10 == 0:\n",
    "        states, actions, rewards, next_states = replay.sampling()\n",
    "        qnet.learn(states, actions, next_states, td_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 0, '  reward:', 0)\n",
      "('episode:', 1, '  reward:', 0)\n",
      "('episode:', 2, '  reward:', 0)\n",
      "('episode:', 3, '  reward:', 0)\n",
      "('episode:', 4, '  reward:', 0)\n",
      "('episode:', 5, '  reward:', 0)\n",
      "('episode:', 6, '  reward:', 0)\n",
      "('episode:', 7, '  reward:', 0)\n",
      "('episode:', 8, '  reward:', 0)\n",
      "('episode:', 9, '  reward:', 0)\n",
      "('episode:', 10, '  reward:', 0)\n",
      "('episode:', 11, '  reward:', 0)\n",
      "('episode:', 12, '  reward:', 0)\n",
      "('episode:', 13, '  reward:', 0)\n",
      "('episode:', 14, '  reward:', 0)\n",
      "('episode:', 15, '  reward:', 0)\n",
      "('episode:', 16, '  reward:', 0)\n",
      "('episode:', 17, '  reward:', 0)\n",
      "('episode:', 18, '  reward:', 0)\n",
      "('episode:', 19, '  reward:', 0)\n",
      "('episode:', 20, '  reward:', 0)\n",
      "('episode:', 21, '  reward:', 0)\n",
      "('episode:', 22, '  reward:', 0)\n",
      "('episode:', 23, '  reward:', 0)\n",
      "('episode:', 24, '  reward:', 0)\n",
      "('episode:', 25, '  reward:', 0)\n",
      "('episode:', 26, '  reward:', 0)\n",
      "('episode:', 27, '  reward:', 0)\n",
      "('episode:', 28, '  reward:', 0)\n",
      "('episode:', 29, '  reward:', 0)\n",
      "('episode:', 30, '  reward:', 0)\n",
      "('episode:', 31, '  reward:', 0)\n",
      "('episode:', 32, '  reward:', 0)\n",
      "('episode:', 33, '  reward:', 0)\n",
      "('episode:', 34, '  reward:', 0)\n",
      "('episode:', 35, '  reward:', 0)\n",
      "('episode:', 36, '  reward:', 0)\n",
      "('episode:', 37, '  reward:', 0)\n",
      "('episode:', 38, '  reward:', 0)\n",
      "('episode:', 39, '  reward:', 0)\n",
      "('episode:', 40, '  reward:', 0)\n",
      "('episode:', 41, '  reward:', 0)\n",
      "('episode:', 42, '  reward:', 0)\n",
      "('episode:', 43, '  reward:', 0)\n",
      "('episode:', 44, '  reward:', 0)\n",
      "('episode:', 45, '  reward:', 0)\n",
      "('episode:', 46, '  reward:', 0)\n",
      "('episode:', 47, '  reward:', 0)\n",
      "('episode:', 48, '  reward:', 0)\n",
      "('episode:', 49, '  reward:', 0)\n",
      "('episode:', 50, '  reward:', 0)\n",
      "('episode:', 51, '  reward:', 0)\n",
      "('episode:', 52, '  reward:', 0)\n",
      "('episode:', 53, '  reward:', 0)\n",
      "('episode:', 54, '  reward:', 0)\n",
      "('episode:', 55, '  reward:', 0)\n",
      "('episode:', 56, '  reward:', 0)\n",
      "('episode:', 57, '  reward:', 0)\n",
      "('episode:', 58, '  reward:', 0)\n",
      "('episode:', 59, '  reward:', 0)\n",
      "('episode:', 60, '  reward:', 0)\n",
      "('episode:', 61, '  reward:', 0)\n",
      "('episode:', 62, '  reward:', 0)\n",
      "('episode:', 63, '  reward:', 0)\n",
      "('episode:', 64, '  reward:', 0)\n",
      "('episode:', 65, '  reward:', 0)\n",
      "('episode:', 66, '  reward:', 0)\n",
      "('episode:', 67, '  reward:', 0)\n",
      "('episode:', 68, '  reward:', 0)\n",
      "('episode:', 69, '  reward:', 0)\n",
      "('episode:', 70, '  reward:', 0)\n",
      "('episode:', 71, '  reward:', 0)\n",
      "('episode:', 72, '  reward:', 0)\n",
      "('episode:', 73, '  reward:', 0)\n",
      "('episode:', 74, '  reward:', 0)\n",
      "('episode:', 75, '  reward:', 0)\n",
      "('episode:', 76, '  reward:', 0)\n",
      "('episode:', 77, '  reward:', 0)\n",
      "('episode:', 78, '  reward:', 0)\n",
      "('episode:', 79, '  reward:', 0)\n",
      "('episode:', 80, '  reward:', 0)\n",
      "('episode:', 81, '  reward:', 0)\n",
      "('episode:', 82, '  reward:', 0)\n",
      "('episode:', 83, '  reward:', 0)\n",
      "('episode:', 84, '  reward:', 0)\n",
      "('episode:', 85, '  reward:', 0)\n",
      "('episode:', 86, '  reward:', 0)\n",
      "('episode:', 87, '  reward:', 0)\n",
      "('episode:', 88, '  reward:', 0)\n",
      "('episode:', 89, '  reward:', 0)\n",
      "('episode:', 90, '  reward:', 0)\n",
      "('episode:', 91, '  reward:', 0)\n",
      "('episode:', 92, '  reward:', 0)\n",
      "('episode:', 93, '  reward:', 0)\n",
      "('episode:', 94, '  reward:', 0)\n",
      "('episode:', 95, '  reward:', 0)\n",
      "('episode:', 96, '  reward:', 0)\n",
      "('episode:', 97, '  reward:', 0)\n",
      "('episode:', 98, '  reward:', 0)\n",
      "('episode:', 99, '  reward:', 0)\n",
      "('episode:', 100, '  reward:', 0)\n",
      "('episode:', 101, '  reward:', 0)\n",
      "('episode:', 102, '  reward:', 0)\n",
      "('episode:', 103, '  reward:', 0)\n",
      "('episode:', 104, '  reward:', 0)\n",
      "('episode:', 105, '  reward:', 0)\n",
      "('episode:', 106, '  reward:', 0)\n",
      "('episode:', 107, '  reward:', 0)\n",
      "('episode:', 108, '  reward:', 0)\n",
      "('episode:', 109, '  reward:', 0)\n",
      "('episode:', 110, '  reward:', 0)\n",
      "('episode:', 111, '  reward:', 0)\n",
      "('episode:', 112, '  reward:', 0)\n",
      "('episode:', 113, '  reward:', 0)\n",
      "('episode:', 114, '  reward:', 0)\n",
      "('episode:', 115, '  reward:', 0)\n",
      "('episode:', 116, '  reward:', 0)\n",
      "('episode:', 117, '  reward:', 0)\n",
      "('episode:', 118, '  reward:', 0)\n",
      "('episode:', 119, '  reward:', 0)\n",
      "('episode:', 120, '  reward:', 0)\n",
      "('episode:', 121, '  reward:', 0)\n",
      "('episode:', 122, '  reward:', 0)\n",
      "('episode:', 123, '  reward:', 0)\n",
      "('episode:', 124, '  reward:', 0)\n",
      "('episode:', 125, '  reward:', 0)\n",
      "('episode:', 126, '  reward:', 0)\n",
      "('episode:', 127, '  reward:', 0)\n",
      "('episode:', 128, '  reward:', 0)\n",
      "('episode:', 129, '  reward:', 0)\n",
      "('episode:', 130, '  reward:', 0)\n",
      "('episode:', 131, '  reward:', 0)\n",
      "('episode:', 132, '  reward:', 0)\n",
      "('episode:', 133, '  reward:', 0)\n",
      "('episode:', 134, '  reward:', 0)\n",
      "('episode:', 135, '  reward:', 0)\n",
      "('episode:', 136, '  reward:', 0)\n",
      "('episode:', 137, '  reward:', 0)\n",
      "('episode:', 138, '  reward:', 0)\n",
      "('episode:', 139, '  reward:', 0)\n",
      "('episode:', 140, '  reward:', 0)\n",
      "('episode:', 141, '  reward:', 0)\n",
      "('episode:', 142, '  reward:', 0)\n",
      "('episode:', 143, '  reward:', 0)\n",
      "('episode:', 144, '  reward:', 0)\n",
      "('episode:', 145, '  reward:', 0)\n",
      "('episode:', 146, '  reward:', 0)\n",
      "('episode:', 147, '  reward:', 0)\n",
      "('episode:', 148, '  reward:', 0)\n",
      "('episode:', 149, '  reward:', 0)\n",
      "('episode:', 150, '  reward:', 0)\n",
      "('episode:', 151, '  reward:', 0)\n",
      "('episode:', 152, '  reward:', 0)\n",
      "('episode:', 153, '  reward:', 0)\n",
      "('episode:', 154, '  reward:', 0)\n",
      "('episode:', 155, '  reward:', 0)\n",
      "('episode:', 156, '  reward:', 0)\n",
      "('episode:', 157, '  reward:', 0)\n",
      "('episode:', 158, '  reward:', 0)\n",
      "('episode:', 159, '  reward:', 0)\n",
      "('episode:', 160, '  reward:', 0)\n",
      "('episode:', 161, '  reward:', 0)\n",
      "('episode:', 162, '  reward:', 0)\n",
      "('episode:', 163, '  reward:', 0)\n",
      "('episode:', 164, '  reward:', 0)\n",
      "('episode:', 165, '  reward:', 0)\n",
      "('episode:', 166, '  reward:', 0)\n",
      "('episode:', 167, '  reward:', 0)\n",
      "('episode:', 168, '  reward:', 0)\n",
      "('episode:', 169, '  reward:', 0)\n",
      "('episode:', 170, '  reward:', 0)\n",
      "('episode:', 171, '  reward:', 0)\n",
      "('episode:', 172, '  reward:', 0)\n",
      "('episode:', 173, '  reward:', 0)\n",
      "('episode:', 174, '  reward:', 0)\n",
      "('episode:', 175, '  reward:', 0)\n",
      "('episode:', 176, '  reward:', 0)\n",
      "('episode:', 177, '  reward:', 0)\n",
      "('episode:', 178, '  reward:', 0)\n",
      "('episode:', 179, '  reward:', 0)\n",
      "('episode:', 180, '  reward:', 0)\n",
      "('episode:', 181, '  reward:', 0)\n",
      "('episode:', 182, '  reward:', 0)\n",
      "('episode:', 183, '  reward:', 0)\n",
      "('episode:', 184, '  reward:', 0)\n",
      "('episode:', 185, '  reward:', 0)\n",
      "('episode:', 186, '  reward:', 0)\n",
      "('episode:', 187, '  reward:', 0)\n",
      "('episode:', 188, '  reward:', 0)\n",
      "('episode:', 189, '  reward:', 0)\n",
      "('episode:', 190, '  reward:', 0)\n",
      "('episode:', 191, '  reward:', 0)\n",
      "('episode:', 192, '  reward:', 0)\n",
      "('episode:', 193, '  reward:', 0)\n",
      "('episode:', 194, '  reward:', 0)\n",
      "('episode:', 195, '  reward:', 0)\n",
      "('episode:', 196, '  reward:', 0)\n",
      "('episode:', 197, '  reward:', 0)\n",
      "('episode:', 198, '  reward:', 0)\n",
      "('episode:', 199, '  reward:', 0)\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(0, 200):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(action_map[a, :])\n",
    "        s = s_\n",
    "        t += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
