{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import gym_coverage\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 50000\n",
    "DISPLAY_REWARD_THRESHOLD = -10  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 500 # maximum time step in one episode\n",
    "RENDER = False # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR = 0.0001    # learning rate\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "env = gym.make('Coverage-v0')\n",
    "env.seed(1)  # reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)  # reproducible\n",
    "\n",
    "N_F_pos = env.observation_space.spaces[0].n\n",
    "N_F_map = env.observation_space.spaces[1].shape[0] * env.observation_space.spaces[1].shape[1]\n",
    "N_F = N_F_pos + N_F_map\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay(object):\n",
    "    def __init__(self, memory_size, state_dim, act_dim, batch_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.actions = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.rewards = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.next_states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "    def fifo(self, state, action, reward, next_state):\n",
    "        self.states[self.current] = state\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.next_states[self.current] = next_state\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        self.count = self.count + 1\n",
    "    def add_trajectory(self, states, actions, rewards, next_states):\n",
    "        num = len(observes)\n",
    "        for i in range(0, num):\n",
    "            self.fifo(states[i], actions[i], rewards[i], next_states[i])\n",
    "    def sampling(self):\n",
    "        indexes = np.random.randint(min(self.count, self.memory_size), size=self.batch_size)\n",
    "        states = self.states[indexes]\n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        next_states = self.next_states[indexes]\n",
    "        states = states.reshape((-1, self.state_dim))\n",
    "        actions = actions.reshape((-1, 1))\n",
    "        rewards = rewards.reshape((-1, 1))\n",
    "        next_states = next_states.reshape((-1, self.state_dim))\n",
    "        return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNET(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, (None, n_features), \"state\")\n",
    "        self.r = tf.placeholder(tf.float32, (None, 1), 'reward')\n",
    "        self.q_ = tf.placeholder(tf.float32, (None, 1), 'q_next')\n",
    "        self.td_error = tf.placeholder(tf.float32, (None, 1), 'td_error')\n",
    "\n",
    "        with tf.variable_scope('Q'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=32,  # number of hidden units\n",
    "                activation=tf.nn.tanh,  # None\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "            \n",
    "            l2 = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=16,  # number of hidden units\n",
    "                activation=tf.nn.tanh,  # None\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l2'\n",
    "            )\n",
    "\n",
    "            self.q = tf.layers.dense(\n",
    "                inputs=l2,\n",
    "                units=n_actions,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='Q'\n",
    "            )\n",
    "        \n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            #self.td_error = self.r + GAMMA * self.q_ - self.q[a]\n",
    "            self.loss = tf.reduce_sum(tf.square(self.td_error))    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "    '''\n",
    "    def learn(self, s, a, r, s_):\n",
    "        BATCH_SIZE = 200\n",
    "        NITER = int(s.shape[0]/BATCH_SIZE)\n",
    "        td_error = self.get_error(s, a, r, s_)\n",
    "        for i in range(0, NITER):\n",
    "            _ = self.sess.run(self.train_op, {self.td_error: td_error[i*BATCH_SIZE:(i+1)*BATCH_SIZE]})\n",
    "        return 0\n",
    "    '''\n",
    "    def get_error(self, s, a, r, s_):\n",
    "        q_ = self.sess.run(self.q, {self.s: s_})\n",
    "        q = self.sess.run(self.q, {self.s: s})\n",
    "        td_error = r + gamma * np.amax(q_, axis=1) - q[a]\n",
    "        BATCH_SIZE = 200\n",
    "        NITER = int(s.shape[0]/BATCH_SIZE)\n",
    "        for i in range(0, NITER):\n",
    "            _ = self.sess.run(self.train_op, {self.td_error: td_error[i*BATCH_SIZE:(i+1)*BATCH_SIZE]})\n",
    "        return 0\n",
    "    \n",
    "    def get_actions(self, s):\n",
    "        q = self.sess.run(self.q, {self.s: s})\n",
    "        action = np.argmax(q, axis=1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Q/l1/kernel:0' shape=(200, 32) dtype=float32_ref>\", \"<tf.Variable 'Q/l1/bias:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/kernel:0' shape=(32, 16) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/bias:0' shape=(16,) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/kernel:0' shape=(16, 4) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/bias:0' shape=(4,) dtype=float32_ref>\"] and loss Tensor(\"squared_TD_error/Sum:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1378539017ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mqnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# we need a good teacher, so the teacher should learn faster than the actor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpReplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEMORY_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8aabf5825465>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, n_features, n_actions, lr)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# TD_error = (r+gamma*V_next) - V_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     '''\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cjg429/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    348\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Q/l1/kernel:0' shape=(200, 32) dtype=float32_ref>\", \"<tf.Variable 'Q/l1/bias:0' shape=(32,) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/kernel:0' shape=(32, 16) dtype=float32_ref>\", \"<tf.Variable 'Q/l2/bias:0' shape=(16,) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/kernel:0' shape=(16, 4) dtype=float32_ref>\", \"<tf.Variable 'Q/Q/bias:0' shape=(4,) dtype=float32_ref>\"] and loss Tensor(\"squared_TD_error/Sum:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "qnet = QNET(sess, n_features=N_F, n_actions=N_A, lr=LR)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "replay = ExpReplay(MEMORY_SIZE, N_F, N_A)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    s = np.concatenate((np.eye(100)[s[0]], s[1].flatten()))\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = qnet.get_actions(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        s_ = np.concatenate((np.eye(100)[s_[0]], s_[1].flatten()))\n",
    "        track_r.append(r)\n",
    "       \n",
    "        replay.fifo(s, a, r, s_)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            running_reward = ep_rs_sum\n",
    "            '''\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            '''\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break\n",
    "        \n",
    "    if (i_episode + 1)%10 == 0:\n",
    "        states, actions, rewards, next_states = replay.sampling()\n",
    "        qnet.learn(states, actions, next_states, td_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(0, 200):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(action_map[a, :])\n",
    "        s = s_\n",
    "        t += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
