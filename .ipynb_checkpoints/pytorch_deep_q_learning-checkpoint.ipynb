{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_coverage\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size=10000, per_alpha=0.2, per_beta0=0.4):       \n",
    "        self.memory = SumTree(capacity=memory_size)\n",
    "        self.memory_size = memory_size\n",
    "        self.per_alpha = per_alpha\n",
    "        self.per_beta0 = per_beta0\n",
    "        self.per_beta = per_beta0\n",
    "        self.per_epsilon = 1E-6\n",
    "        self.prio_max = 0\n",
    "    \n",
    "    def anneal_per_importance_sampling(self, step, max_step):\n",
    "        self.per_beta = self.per_beta0 + step*(1-self.per_beta0)/max_step\n",
    "\n",
    "    def error2priority(self, errors):\n",
    "        return np.power(np.abs(errors) + self.per_epsilon, self.per_alpha)\n",
    "\n",
    "    def save_experience(self, state, action, reward, state_next, done):\n",
    "        experience = (state, action, reward, state_next, done)\n",
    "        self.memory.add(np.max([self.prio_max, self.per_epsilon]), experience)\n",
    "        \n",
    "    def retrieve_experience(self, batch_size):\n",
    "        idx = None\n",
    "        priorities = None\n",
    "        w = None\n",
    "\n",
    "        idx, priorities, experience = self.memory.sample(batch_size)\n",
    "        sampling_probabilities = priorities / self.memory.total()\n",
    "        w = np.power(self.memory.n_entries * sampling_probabilities, -self.per_beta)\n",
    "        w = w / w.max()\n",
    "        return idx, priorities, w, experience\n",
    "    \n",
    "    def update_experience_weight(self, idx, errors ):\n",
    "        priorities = self.error2priority(errors)\n",
    "        for i in range(len(idx)):\n",
    "            self.memory.update(idx[i], priorities[i])\n",
    "        self.prio_max = max(priorities.max(), self.prio_max)\n",
    "        \n",
    "class SumTree:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2*capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "        self.write = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "        self.tree_len = len(self.tree)\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "\n",
    "        if left >= self.tree_len:\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            right = left + 1\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        data_idx = idx - self.capacity + 1\n",
    "\n",
    "        return idx, self.tree[idx], self.data[data_idx]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_idx = [None] * batch_size\n",
    "        batch_priorities = [None] * batch_size\n",
    "        batch = [None] * batch_size\n",
    "        segment = self.total() / batch_size\n",
    "\n",
    "        a = [segment*i for i in range(batch_size)]\n",
    "        b = [segment * (i+1) for i in range(batch_size)]\n",
    "        s = np.random.uniform(a, b)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            (batch_idx[i], batch_priorities[i], batch[i]) = self.get(s[i])\n",
    "\n",
    "        return batch_idx, batch_priorities, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, obs_dim, n_action, seed=0,\n",
    "                 discount_factor = 0.995, epsilon_decay = 0.999, epsilon_min = 0.01,\n",
    "                 learning_rate = 1e-3, # STEP SIZE\n",
    "                 batch_size = 64, \n",
    "                 memory_size = 10000, hidden_unit_size = 64,\n",
    "                 target_mode = 'DDQN', memory_mode = 'PER'):\n",
    "        self.seed = seed \n",
    "        self.obs_dim = obs_dim\n",
    "        self.n_action = n_action\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.train_start = 5000\n",
    "\n",
    "        self.target_mode = target_mode\n",
    "        self.memory_mode = memory_mode\n",
    "        if memory_mode == 'PER':\n",
    "            self.memory = ReplayMemory(memory_size=memory_size)\n",
    "        else:\n",
    "            self.memory = deque(maxlen=memory_size)\n",
    "            \n",
    "        self.hidden_unit_size = hidden_unit_size\n",
    "        \n",
    "        self.build_model()\n",
    "        #self.build_loss()\n",
    "        #self.build_update_operation()   \n",
    "    \n",
    "        #self.batch_wegihts_ph = \n",
    "        #self.learning_rate_ph =\n",
    "    \n",
    "    def build_model(self):\n",
    "        hid1_size = self.hidden_unit_size  # 10 empirically determined\n",
    "        hid2_size = self.hidden_unit_size\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(self.obs_dim, hid1_size, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(hid1_size, hid2_size, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(hid2_size, self.n_action, bias=True)\n",
    "        tanh = torch.nn.Tanh()\n",
    "\n",
    "        self.q_predict = torch.nn.Sequential(self.fc1, tanh, self.fc2, tanh, self.fc3)\n",
    "        \n",
    "        self.fc1_old = torch.nn.Linear(self.obs_dim, hid1_size, bias=True)\n",
    "        self.fc2_old = torch.nn.Linear(hid1_size, hid2_size, bias=True)\n",
    "        self.fc3_old = torch.nn.Linear(hid2_size, self.n_action, bias=True)\n",
    "\n",
    "        self.q_predict_old = torch.nn.Sequential(self.fc1_old, tanh, self.fc2_old, tanh, self.fc3_old)\n",
    "    '''\n",
    "    def build_loss(self):\n",
    "            \n",
    "    def build_update_operation(self):\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \n",
    "    def restore_model(self, path):\n",
    "    '''    \n",
    "    def update_target(self):\n",
    "        self.fc1_old.weight = self.fc1.weight\n",
    "        self.fc1_old.bias = self.fc1.bias\n",
    "        self.fc2_old.weight = self.fc2.weight\n",
    "        self.fc2_old.bias = self.fc2.bias\n",
    "        self.fc3_old.weight = self.fc3.weight\n",
    "        self.fc3_old.bias = self.fc3.bias\n",
    "        \n",
    "    def update_memory(self, step, max_step):\n",
    "        if self.memory_mode == 'PER':\n",
    "            self.memory.anneal_per_importance_sampling(step, max_step)\n",
    "        \n",
    "    def update_policy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def get_prediction_old(self, obs): \n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        q_value_old = self.q_predict_old(Variable(obs))\n",
    "        return q_value_old.float()\n",
    "        \n",
    "    def get_prediction(self, obs):\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        q_value = self.q_predict(Variable(obs))            \n",
    "        return q_value.float()\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #return random.randint(0, 3)\n",
    "            return random.randrange(self.n_action)\n",
    "        else:\n",
    "            q_value = self.get_prediction(obs).data.numpy()\n",
    "            return np.argmax(q_value[0])\n",
    "    \n",
    "    def add_experience(self, obs, action, reward, next_obs, done):\n",
    "        if self.memory_mode == 'PER':\n",
    "            self.memory.save_experience(obs, action, reward, next_obs, done)\n",
    "        else:\n",
    "            self.memory.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        output = np.nan\n",
    "        \n",
    "        if self.memory_mode == 'PER':\n",
    "            n_entries = self.memory.memory.n_entries\n",
    "        else:\n",
    "            n_entries = len(self.memory)\n",
    "            \n",
    "        if n_entries > self.train_start:\n",
    "            \n",
    "            if self.memory_mode == 'PER':\n",
    "                # PRIORITIZED EXPERIENCE REPLAY\n",
    "                idx, priorities, w, mini_batch = self.memory.retrieve_experience(self.batch_size)\n",
    "                batch_weights = np.transpose(np.tile(w, (self.n_action, 1)))\n",
    "            else:\n",
    "                mini_batch = random.sample(self.memory, self.batch_size)\n",
    "                batch_weights = np.ones((self.batch_size,self.n_action))\n",
    "\n",
    "            observations = np.zeros((self.batch_size, self.obs_dim))\n",
    "            next_observations = np.zeros((self.batch_size, self.obs_dim))\n",
    "            actions, rewards, dones = [], [], []\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                observations[i] = mini_batch[i][0]\n",
    "                actions.append(mini_batch[i][1])\n",
    "                rewards.append(mini_batch[i][2])\n",
    "                next_observations[i] = mini_batch[i][3]\n",
    "                dones.append(mini_batch[i][4])\n",
    "            \n",
    "            target = self.get_prediction(observations)\n",
    "            target = target.data.numpy()\n",
    "            if self.target_mode == 'DDQN':\n",
    "                bast_a = np.argmax(self.get_prediction(next_observations), axis=1)\n",
    "            next_q_value = self.get_prediction_old(next_observations)\n",
    "            next_q_value = next_q_value.data.numpy()\n",
    "\n",
    "            # BELLMAN UPDATE RULE \n",
    "            for i in range(self.batch_size):\n",
    "                if dones[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    if self.target_mode == 'DDQN':\n",
    "                        target[i][actions[i]] = rewards[i] + self.discount_factor * next_q_value[i][bast_a[i]]\n",
    "                    else:\n",
    "                        target[i][actions[i]] = rewards[i] + self.discount_factor * (np.amax(next_q_value[i]))\n",
    "\n",
    "            q_predict = self.get_prediction(observations)\n",
    "            target = torch.from_numpy(target).float()\n",
    "            errors = target - q_predict.data\n",
    "            loss = torch.nn.MSELoss()\n",
    "            output = 0.5 * loss(q_predict, Variable(target))\n",
    "            optimizer = torch.optim.Adam(self.q_predict.parameters(), lr=self.learning_rate)\n",
    "            optimizer.zero_grad()\n",
    "            output.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            errors = errors.numpy()\n",
    "            errors = errors[np.arange(len(errors)), actions]\n",
    "            \n",
    "            if self.memory_mode == 'PER':\n",
    "                # PRIORITIZED EXPERIENCE REPLAY\n",
    "                self.memory.update_experience_weight(idx, errors)\n",
    "                \n",
    "            output = output.data.numpy()    \n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Coverage-v0')\n",
    "env.seed(seed)\n",
    "N_F_pos = env.observation_space.spaces[0].n\n",
    "N_F_map = env.observation_space.spaces[1].shape[0] * env.observation_space.spaces[1].shape[1]\n",
    "N_F = N_F_pos + N_F_map\n",
    "N_A = env.action_space.n\n",
    "agent = DQNAgent(N_F, N_A, epsilon_decay= 0.999995, discount_factor=0.999, memory_mode='PER',target_mode='DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss : nan, return : -1001.000, eps : 1.000\n",
      "1000 loss : nan, return : -1004.400, eps : 0.978\n",
      "2000 loss : 174995.531, return : -1003.400, eps : 0.957\n",
      "3000 loss : 37574.988, return : -1001.300, eps : 0.937\n",
      "4000 loss : 10060.311, return : -1002.700, eps : 0.917\n",
      "5000 loss : 0.954, return : -1002.700, eps : 0.897\n",
      "6000 loss : 0.012, return : -1002.500, eps : 0.875\n",
      "7000 loss : 0.010, return : -1002.100, eps : 0.855\n",
      "8000 loss : 0.017, return : -1004.900, eps : 0.834\n",
      "9000 loss : 0.020, return : -1005.100, eps : 0.813\n",
      "10000 loss : 0.034, return : -1005.300, eps : 0.793\n",
      "11000 loss : 0.060, return : -1004.300, eps : 0.774\n",
      "12000 loss : 0.149, return : -1005.300, eps : 0.753\n",
      "13000 loss : 0.310, return : -1008.800, eps : 0.731\n",
      "14000 loss : 15.914, return : -1009.500, eps : 0.703\n",
      "15000 loss : 794.355, return : -1008.900, eps : 0.669\n",
      "full cover\n",
      "16000 loss : 135715.078, return : -1014.300, eps : 0.629\n",
      "full cover\n",
      "full cover\n",
      "17000 loss : 171657.250, return : -1011.700, eps : 0.588\n",
      "full cover\n",
      "18000 loss : 99744.148, return : -1006.400, eps : 0.556\n",
      "19000 loss : 144811.500, return : -1014.700, eps : 0.526\n",
      "full cover\n",
      "20000 loss : 31432.895, return : -1006.500, eps : 0.492\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "21000 loss : 32810.312, return : -1013.200, eps : 0.450\n",
      "full cover\n",
      "22000 loss : 33196.680, return : -1034.800, eps : 0.400\n",
      "full cover\n",
      "full cover\n",
      "23000 loss : 6455.085, return : -1017.300, eps : 0.353\n",
      "full cover\n",
      "24000 loss : 4415.160, return : -1026.500, eps : 0.321\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "25000 loss : 1991.770, return : -1013.600, eps : 0.293\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "26000 loss : 5328.313, return : -1010.200, eps : 0.273\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "27000 loss : 2138.394, return : -1010.300, eps : 0.254\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "28000 loss : 9072.862, return : -1017.600, eps : 0.235\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "29000 loss : 3460.675, return : -1007.500, eps : 0.217\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "30000 loss : 6916.034, return : -1019.500, eps : 0.199\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "31000 loss : 4593.985, return : -1026.200, eps : 0.180\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "32000 loss : 4943.687, return : -1025.800, eps : 0.162\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "33000 loss : 657.274, return : -1016.000, eps : 0.143\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "34000 loss : 9455.645, return : -1017.100, eps : 0.124\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "35000 loss : 6422.368, return : -1041.300, eps : 0.107\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "36000 loss : 11659.186, return : -1043.300, eps : 0.087\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "37000 loss : 8577.966, return : -1049.300, eps : 0.067\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "38000 loss : 12315.264, return : -1085.700, eps : 0.049\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "39000 loss : 8565.640, return : -600.200, eps : 0.030\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n",
      "full cover\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3bef02b78014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnext_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1e8c70c710cb>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1e8c70c710cb>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/soft_ac/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/soft_ac/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/soft_ac/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/soft_ac/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/soft_ac/lib/python3.5/site-packages/torch/nn/backends/backend.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_t = 100\n",
    "avg_return_list = deque(maxlen=10)\n",
    "avg_loss_list = deque(maxlen=10)\n",
    "for i in range(300000):\n",
    "    obs = env.reset()\n",
    "    obs = np.concatenate((np.eye(16)[obs[0]], obs[1].flatten()))\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    for t in range(max_t):\n",
    "        action = agent.get_action(np.reshape(obs, (1, -1)))\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        next_obs = np.concatenate((np.eye(16)[next_obs[0]], next_obs[1].flatten()))\n",
    "        agent.add_experience(obs, action, reward, next_obs, done)\n",
    "        \n",
    "        loss = agent.train_model()\n",
    "        agent.update_memory(t, max_t)\n",
    "        agent.update_policy()\n",
    "        \n",
    "        #if i % 10000 == 0:\n",
    "        #    env.render()\n",
    "        #    time.sleep(0.5)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        total_loss += loss\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    agent.update_target()\n",
    "    avg_return_list.append(total_reward)\n",
    "    avg_loss_list.append(total_loss)\n",
    "    \n",
    "    if (i%1000)==0:\n",
    "        print('{} loss : {:.3f}, return : {:.3f}, eps : {:.3f}'.format(i, np.mean(avg_loss_list), np.mean(avg_return_list), agent.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n",
      "14\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    obs = env.reset()\n",
    "    obs = np.concatenate((np.eye(16)[obs[0]], obs[1].flatten()))\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    env.render()\n",
    "    time.sleep(1.0)\n",
    "    for t in range(max_t):\n",
    "        action = agent.get_action(np.reshape(obs, (1, -1)))\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        next_obs = np.concatenate((np.eye(16)[next_obs[0]], next_obs[1].flatten()))\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            env.render()\n",
    "            time.sleep(1.0)\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
